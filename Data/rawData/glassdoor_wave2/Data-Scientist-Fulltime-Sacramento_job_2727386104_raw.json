{"jobID": "2727386104", "jobLocation": "Sacramento_CA", "jobTitle": "Big Data Engineer", "companyRating": "3.5", "companyInfo": {"Website": "www.peoplefinders.com", "Headquarters": "Sacramento, CA", "Size": "1 to 50 employees", "Founded": " Unknown", "Type": " Contract", "Industry": " Media", "Revenue": " Unknown / Non-Applicable per year", "Competitors": " Unknown"}, "estimatedSalary": "68000", "jobDescription": " Who We Want  Are you an early adopter and contributor to the Hadoop Open Source project? Do you live for working on challenging Big Data problems at a massive scale? Are you the type that intimately knows the  ins and outs of Hadoop data development with the expert knowledge and experience to push your hardware to the limits? If yes, then we want you.  We are looking for a Big Data Engineer to help our engineering team build a modern data processing platform in Hadoop and support our existing relational databases. We are investing resources into  setting up a more flexible and scalable data infrastructure to support the addition of new data sets and improve overall data quality. An ideal candidate will be excited to be in a small size company with a  startup mindset that moves quickly on a constant flow of ideas, is able to weed through the maze of Big data tools and potential approaches to find the best possible solution and architecture. ResponsibilitiesImplement and maintain big data platform and infrastructureDevelop, optimize and tune MySQL stored procedures, scripts, and indexesDevelop Hive schemas and scripts, Pig scripts, custom MapReduce and UDFs in JavaDesign, develop and maintain automated, complex, and efficient ETL processes to do batch records-matching of multiple large-scale datasets, including supporting documentationDevelop tools to monitor, debug, and analyze data pipelinesTroubleshoot Hadoop cluster and query issues, evaluate query plans, and optimize schemas and queriesStrong interpersonal skills to resolve problems in a professional manner, lead working groups, and negotiate consensus Qualifications & Skills BS, MS, or PhD in Computer Science or related field, or work experience3+ years minimum experience in language such as Java, Scala, Perl, Shell Scripting or PythonWorking knowledge of the Hadoop ecosystem applications (MapReduce, YARN, Pig, Hbase, Hive, Oozie, Sqoop, Spark and more!)Strong Experience working with data pipelines in multi-terabyte data warehouses. Experience in dealing with performance and scalability issuesStrong SQL (MySQL, Hive, etc.) and No-SQL (MongoDB, Hbase, etc.) skills, including writing complex queries and performance tuningKnowledge of data modeling, partitioning, indexing, and architectural database design.Experience using Source Code and Version Control systems like GIT etc.Experience on continuous build and test process using tools such as Maven, Gradle, Teamcity or JenkinsExperience with Search Engines, Name/Address Matching, or Linux text processingPreferred:Knowledge of cluster configuration, Hadoop administration and performance tuning are a huge plus.Distributed computing principles and experience in big data technologies including performance tuningExposure to Stream processing using Kafka, Flink, Storm or Spark Streaming - provided by Dice MySQL, Hive, java, perl, python, hadoop"}