{"jobID": "2658496632", "jobLocation": "Hoboken_NJ", "jobTitle": "Senior Data Engineer, Big Data Solutions", "companyRating": "3.0", "companyInfo": {"Website": "www.wiley.com", "Headquarters": "Hoboken, NJ", "Size": "5001 to 10000 employees", "Founded": " 1807", "Type": " Company - Public (JW-A) ", "Industry": " Media", "Revenue": " $1 to $2 billion (USD) per year", "Competitors": " Unknown"}, "estimatedSalary": "115000", "jobDescription": "Senior Data Engineer, Big Data SolutionsWiley - HobokenThe Company: We may have been founded over two centuries ago, but our secret to success remains the same: change with the times and adapt to meet the ever-evolving needs of our customers. As a learning business, our mission is to help people and organizations to develop the skills and knowledge they need to succeed. With new products and services driving our global expansion, we\u2019re looking towards a digital future. It\u2019s resulted in a period of true cultural change here at Wiley, which means even more career and development opportunities for our talented people.  The primary focus of this role is to design and build solutions that include data acquisition, storage, transformation, security, data management and data analysis using Amazon Web Services (AWS). This will involve extracting, transforming, and loading data from variety of data sources using different protocols (JSON/REST APIs/JMS/ NoSQL/Flat Files etc.) to the Data Lake, Data Warehouse and Data Marts. In order to be successful in this role, the selected candidate must be comfortable with performing the necessary analysis to extract directly from wide range of data sources. Essential responsibilities for this position include the following: design and development of Data Integration programs, maintenance, enhancement design and implementation, upgrades, testing, performance tuning and optimization. Additionally, the individual must perform functions such as: ensuring application availability and performance, identifying and resolving issues, data reconciliation, process and scalability improvement opportunities, test plan creation, technical and training documentation, and training support. This individual must also champion DevOps approaches by working with other developers to build automated Continuous Integration, Delivery & Deployment pipelines and automating the monitoring and operational needs of the AWS environment. Responsibilities:Design and develop new source system integrations from a variety of formats including files, database extracts and APIs. Design and develop highly scalable Data Pipelines that incorporate complex transformations and efficient code. Data will need to flow to and from unstructured and relational systems for analytic processing. Design and develop solutions for delivering data that meets SLAs and is of high quality to various WB divisions for marketing and reporting as well as external vendors.Collaborate with architects and other data engineers to recommend and design data storage solutions, security, and service level requirements. Collaborate with application and architecture teams to build proof of concepts (POCs) to evaluate new AWS capabilities, technologies.Development of automated solutions to monitor and support AWS environment and its ecosystem. Develop automation and testing protocols or plan for testing application migration and environment test results.Responsible for the day to day operations of the AWS based Big Data Solutions and managing and delivering on SLAs related to information ingestion/transformations, data services, applications and platform servicesInvestigate problems and resolve as required, including working with various internal teams and vendors. Proactively monitor the data flows with a focus on continued performance improvements.Evangelize and educate colleagues about various AWS Big Data technologies; Facilitate hands-on training/working sessions for other Data Engineers within the department.Design, document, and maintain architecture diagrams for systems, processes, and interfaces. Collaborate with other team members in the design, implementation and documentation of solutions for daily issues/support, release management, and new projects.Requirements:B.S Computer Science or related degreeA minimum of 5 or more years of data warehouse design/development experience with at least 3 plus years of AWS experience including S3, EC2, Glue, Kinesis, SQS, Lambda, Redshift, NoSQL databases, EMR, Hcatalog, Hive, Spark, Elastic Cache and Elastic Search.Experience with very large datasets and complex ETL processes involving a diverse set of data sourcesExperience in building data lakes, data marts, data warehouses using structured and semi-structured data sources using AWS ServicesExperience with building data streaming applications based on KinesisProven track record of Architecting Distributed Solutions dealing with real high volume of data.Working knowledge of web technologies and protocols (JSON/REST APIs/JMS/ NoSQL).Experience building and managing complex products and solutions.Experience monitoring, logging, developing KPI\u2019s on system performance and scalability.Experience in agile software development and using repositories like Git. Experience with continuous integration and deployment pipelines leveraging tools like Jenkins.Working knowledge of infrastructure provisioning automation tools, such as Terraform, Cloudformation and Ansible.Ability to keep current with the latest AWS service offerings.Experience with variety of data stores including AWS S3, RedShift, Postgres, NoSQL (DynamoDB or MongoDB or Couchbase) and Elastic Cache.Experience/Knowledge of Cognos and/or Qlikview or other visualization tools like Tableau, PowerBI, SpotFireExpertise in developing Unix Shell Scripts to automate Data Integration processesFormal project methodology framework (SDLC)Build management, release management and version controlStrong verbal and written communication skillsAnalytical and problem solving abilitySelf-motivatedAbility to maintain technical proficiencyAbility to multi-taskDemonstrated ability to work well under time constraints.Collaborate with consultants to deliver projectsTranslation of business requirements into technical specification documentationReview of technology deliverables and testing strategiesProvide triage and defect resolutionProvide technical mentoring to junior staffDesired Skills:More than 2 years of experience in education/publishing industry.Experience in implementing analytics solutions for Learning Management Systems and/or Caliper Analytics.AWS certification/s preferred (Solution Architect and/or Big Data).Proficiency in in one or more of the following languages: Python/PySpark, R/SAS, Scala.Experience in developing data integration programs using Informatica PowerCenter or Talend.Experience with Cognos, QlikView/QlikSense or other visualization tools like Tableau, PowerBI.Hands-on experience in developing Dimensional Data Models and designing Enterprise Data Warehouses. Wiley is an equal opportunity employer and does not discriminate on the basis on race, color, creed, national origin, sex, sexual orientation, religion, age, disability or other legally protected status. Employment is contingent upon the successful completion of a background check and employment review."}